{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.50  Python-3.9.13 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12287MiB)\n",
      "Setup complete  (16 CPUs, 31.9 GB RAM, 204.2/265.6 GB disk)\n"
     ]
    }
   ],
   "source": [
    "#%pip install git+https://github.com/ultralytics/ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prunining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model saved at: Model/Model_M_L1_Pruned.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Define model paths\n",
    "original_model_path = \"Model/Model_M_best.pt\"\n",
    "pruned_model_path = \"Model/Model_M_L1_Pruned.pt\"\n",
    "\n",
    "# Load model\n",
    "model = YOLO(original_model_path)\n",
    "\n",
    "# Identify Conv2d layers for pruning\n",
    "parameters_to_prune = [(m, \"weight\") for _, m in model.model.named_modules() if isinstance(m, torch.nn.Conv2d)]\n",
    "\n",
    "# Apply global unstructured pruning (L1 Norm, 51%)\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.51,\n",
    ")\n",
    "\n",
    "# Make pruning permanent\n",
    "for module, param_name in parameters_to_prune:\n",
    "    prune.remove(module, param_name)\n",
    "\n",
    "# Save pruned model\n",
    "model.save(pruned_model_path)\n",
    "\n",
    "# Free up memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Pruned model saved at: {pruned_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ultralytics in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (8.3.50)\n",
      "Requirement already satisfied: onnx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxruntime-gpu in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (1.26.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from ultralytics) (3.8.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (0.21.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ultralytics) (2.0.13)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime-gpu) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from onnxruntime-gpu) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=3.3.0->ultralytics) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime-gpu) (3.5.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics onnx onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export YOLO Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.50  Python-3.9.13 torch-2.6.0+cu118 CPU (AMD Ryzen 7 5700X 8-Core Processor)\n",
      "YOLO11m summary (fused): 303 layers, 20,036,971 parameters, 0 gradients, 67.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'Model\\Model_M_L1_Pruned.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 13, 8400) (38.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.8s, saved as 'Model\\Model_M_L1_Pruned.onnx' (76.7 MB)\n",
      "\n",
      "Export complete (2.6s)\n",
      "Results saved to \u001b[1mG:\\Shahadath\\Model\u001b[0m\n",
      "Predict:         yolo predict task=detect model=Model\\Model_M_L1_Pruned.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=Model\\Model_M_L1_Pruned.onnx imgsz=640 data=/kaggle/working/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      " ONNX model exported successfully with fixed input shapes.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"Model/Model_M_L1_Pruned.pt\")\n",
    "\n",
    "# Define static input shape (batch_size=1, channels=3, height=640, width=640)\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "# Export to ONNX with fixed input shape\n",
    "model.export(format=\"onnx\", imgsz=640, dynamic=False, simplify=True)\n",
    "\n",
    "print(\" ONNX model exported successfully with fixed input shapes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert ONNX to TensorRT FP16 Engine by the Command Prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trtexec --onnx=G:\\Shahadath\\Model\\Model_M_L1_Pruned.onnx --saveEngine=G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine --fp16 --useSpinWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Run YOLO Model with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'boneanomaly', 1: 'bonelesion', 2: 'foreignbody', 3: 'fracture', 4: 'metal', 5: 'periostealreaction', 6: 'pronatorsign', 7: 'softtissue', 8: 'text'}\n",
      "obb: None\n",
      "orig_img: array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8)\n",
      "orig_shape: (794, 496)\n",
      "path: 'G:\\\\Shahadath\\\\Dataset\\\\test\\\\images\\\\5096_0745204059_03_WRI-R1_F010.png'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 2.0051002502441406, 'inference': 27.550220489501953, 'postprocess': 1.005411148071289}]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the optimized TensorRT model\n",
    "model = YOLO(\"G:/Shahadath/Model/Model_M_best.pt\")\n",
    "\n",
    "# Run inference on an image\n",
    "results = model(\"G:/Shahadath/Dataset/test/images/5096_0745204059_03_WRI-R1_F010.png\", verbose=False)\n",
    "\n",
    "# Display results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine for TensorRT inference...\n",
      "WARNING  Metadata not found for 'model=G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'class0', 1: 'class1', 2: 'class2', 3: 'class3', 4: 'class4', 5: 'class5', 6: 'class6', 7: 'class7', 8: 'class8', 9: 'class9', 10: 'class10', 11: 'class11', 12: 'class12', 13: 'class13', 14: 'class14', 15: 'class15', 16: 'class16', 17: 'class17', 18: 'class18', 19: 'class19', 20: 'class20', 21: 'class21', 22: 'class22', 23: 'class23', 24: 'class24', 25: 'class25', 26: 'class26', 27: 'class27', 28: 'class28', 29: 'class29', 30: 'class30', 31: 'class31', 32: 'class32', 33: 'class33', 34: 'class34', 35: 'class35', 36: 'class36', 37: 'class37', 38: 'class38', 39: 'class39', 40: 'class40', 41: 'class41', 42: 'class42', 43: 'class43', 44: 'class44', 45: 'class45', 46: 'class46', 47: 'class47', 48: 'class48', 49: 'class49', 50: 'class50', 51: 'class51', 52: 'class52', 53: 'class53', 54: 'class54', 55: 'class55', 56: 'class56', 57: 'class57', 58: 'class58', 59: 'class59', 60: 'class60', 61: 'class61', 62: 'class62', 63: 'class63', 64: 'class64', 65: 'class65', 66: 'class66', 67: 'class67', 68: 'class68', 69: 'class69', 70: 'class70', 71: 'class71', 72: 'class72', 73: 'class73', 74: 'class74', 75: 'class75', 76: 'class76', 77: 'class77', 78: 'class78', 79: 'class79', 80: 'class80', 81: 'class81', 82: 'class82', 83: 'class83', 84: 'class84', 85: 'class85', 86: 'class86', 87: 'class87', 88: 'class88', 89: 'class89', 90: 'class90', 91: 'class91', 92: 'class92', 93: 'class93', 94: 'class94', 95: 'class95', 96: 'class96', 97: 'class97', 98: 'class98', 99: 'class99', 100: 'class100', 101: 'class101', 102: 'class102', 103: 'class103', 104: 'class104', 105: 'class105', 106: 'class106', 107: 'class107', 108: 'class108', 109: 'class109', 110: 'class110', 111: 'class111', 112: 'class112', 113: 'class113', 114: 'class114', 115: 'class115', 116: 'class116', 117: 'class117', 118: 'class118', 119: 'class119', 120: 'class120', 121: 'class121', 122: 'class122', 123: 'class123', 124: 'class124', 125: 'class125', 126: 'class126', 127: 'class127', 128: 'class128', 129: 'class129', 130: 'class130', 131: 'class131', 132: 'class132', 133: 'class133', 134: 'class134', 135: 'class135', 136: 'class136', 137: 'class137', 138: 'class138', 139: 'class139', 140: 'class140', 141: 'class141', 142: 'class142', 143: 'class143', 144: 'class144', 145: 'class145', 146: 'class146', 147: 'class147', 148: 'class148', 149: 'class149', 150: 'class150', 151: 'class151', 152: 'class152', 153: 'class153', 154: 'class154', 155: 'class155', 156: 'class156', 157: 'class157', 158: 'class158', 159: 'class159', 160: 'class160', 161: 'class161', 162: 'class162', 163: 'class163', 164: 'class164', 165: 'class165', 166: 'class166', 167: 'class167', 168: 'class168', 169: 'class169', 170: 'class170', 171: 'class171', 172: 'class172', 173: 'class173', 174: 'class174', 175: 'class175', 176: 'class176', 177: 'class177', 178: 'class178', 179: 'class179', 180: 'class180', 181: 'class181', 182: 'class182', 183: 'class183', 184: 'class184', 185: 'class185', 186: 'class186', 187: 'class187', 188: 'class188', 189: 'class189', 190: 'class190', 191: 'class191', 192: 'class192', 193: 'class193', 194: 'class194', 195: 'class195', 196: 'class196', 197: 'class197', 198: 'class198', 199: 'class199', 200: 'class200', 201: 'class201', 202: 'class202', 203: 'class203', 204: 'class204', 205: 'class205', 206: 'class206', 207: 'class207', 208: 'class208', 209: 'class209', 210: 'class210', 211: 'class211', 212: 'class212', 213: 'class213', 214: 'class214', 215: 'class215', 216: 'class216', 217: 'class217', 218: 'class218', 219: 'class219', 220: 'class220', 221: 'class221', 222: 'class222', 223: 'class223', 224: 'class224', 225: 'class225', 226: 'class226', 227: 'class227', 228: 'class228', 229: 'class229', 230: 'class230', 231: 'class231', 232: 'class232', 233: 'class233', 234: 'class234', 235: 'class235', 236: 'class236', 237: 'class237', 238: 'class238', 239: 'class239', 240: 'class240', 241: 'class241', 242: 'class242', 243: 'class243', 244: 'class244', 245: 'class245', 246: 'class246', 247: 'class247', 248: 'class248', 249: 'class249', 250: 'class250', 251: 'class251', 252: 'class252', 253: 'class253', 254: 'class254', 255: 'class255', 256: 'class256', 257: 'class257', 258: 'class258', 259: 'class259', 260: 'class260', 261: 'class261', 262: 'class262', 263: 'class263', 264: 'class264', 265: 'class265', 266: 'class266', 267: 'class267', 268: 'class268', 269: 'class269', 270: 'class270', 271: 'class271', 272: 'class272', 273: 'class273', 274: 'class274', 275: 'class275', 276: 'class276', 277: 'class277', 278: 'class278', 279: 'class279', 280: 'class280', 281: 'class281', 282: 'class282', 283: 'class283', 284: 'class284', 285: 'class285', 286: 'class286', 287: 'class287', 288: 'class288', 289: 'class289', 290: 'class290', 291: 'class291', 292: 'class292', 293: 'class293', 294: 'class294', 295: 'class295', 296: 'class296', 297: 'class297', 298: 'class298', 299: 'class299', 300: 'class300', 301: 'class301', 302: 'class302', 303: 'class303', 304: 'class304', 305: 'class305', 306: 'class306', 307: 'class307', 308: 'class308', 309: 'class309', 310: 'class310', 311: 'class311', 312: 'class312', 313: 'class313', 314: 'class314', 315: 'class315', 316: 'class316', 317: 'class317', 318: 'class318', 319: 'class319', 320: 'class320', 321: 'class321', 322: 'class322', 323: 'class323', 324: 'class324', 325: 'class325', 326: 'class326', 327: 'class327', 328: 'class328', 329: 'class329', 330: 'class330', 331: 'class331', 332: 'class332', 333: 'class333', 334: 'class334', 335: 'class335', 336: 'class336', 337: 'class337', 338: 'class338', 339: 'class339', 340: 'class340', 341: 'class341', 342: 'class342', 343: 'class343', 344: 'class344', 345: 'class345', 346: 'class346', 347: 'class347', 348: 'class348', 349: 'class349', 350: 'class350', 351: 'class351', 352: 'class352', 353: 'class353', 354: 'class354', 355: 'class355', 356: 'class356', 357: 'class357', 358: 'class358', 359: 'class359', 360: 'class360', 361: 'class361', 362: 'class362', 363: 'class363', 364: 'class364', 365: 'class365', 366: 'class366', 367: 'class367', 368: 'class368', 369: 'class369', 370: 'class370', 371: 'class371', 372: 'class372', 373: 'class373', 374: 'class374', 375: 'class375', 376: 'class376', 377: 'class377', 378: 'class378', 379: 'class379', 380: 'class380', 381: 'class381', 382: 'class382', 383: 'class383', 384: 'class384', 385: 'class385', 386: 'class386', 387: 'class387', 388: 'class388', 389: 'class389', 390: 'class390', 391: 'class391', 392: 'class392', 393: 'class393', 394: 'class394', 395: 'class395', 396: 'class396', 397: 'class397', 398: 'class398', 399: 'class399', 400: 'class400', 401: 'class401', 402: 'class402', 403: 'class403', 404: 'class404', 405: 'class405', 406: 'class406', 407: 'class407', 408: 'class408', 409: 'class409', 410: 'class410', 411: 'class411', 412: 'class412', 413: 'class413', 414: 'class414', 415: 'class415', 416: 'class416', 417: 'class417', 418: 'class418', 419: 'class419', 420: 'class420', 421: 'class421', 422: 'class422', 423: 'class423', 424: 'class424', 425: 'class425', 426: 'class426', 427: 'class427', 428: 'class428', 429: 'class429', 430: 'class430', 431: 'class431', 432: 'class432', 433: 'class433', 434: 'class434', 435: 'class435', 436: 'class436', 437: 'class437', 438: 'class438', 439: 'class439', 440: 'class440', 441: 'class441', 442: 'class442', 443: 'class443', 444: 'class444', 445: 'class445', 446: 'class446', 447: 'class447', 448: 'class448', 449: 'class449', 450: 'class450', 451: 'class451', 452: 'class452', 453: 'class453', 454: 'class454', 455: 'class455', 456: 'class456', 457: 'class457', 458: 'class458', 459: 'class459', 460: 'class460', 461: 'class461', 462: 'class462', 463: 'class463', 464: 'class464', 465: 'class465', 466: 'class466', 467: 'class467', 468: 'class468', 469: 'class469', 470: 'class470', 471: 'class471', 472: 'class472', 473: 'class473', 474: 'class474', 475: 'class475', 476: 'class476', 477: 'class477', 478: 'class478', 479: 'class479', 480: 'class480', 481: 'class481', 482: 'class482', 483: 'class483', 484: 'class484', 485: 'class485', 486: 'class486', 487: 'class487', 488: 'class488', 489: 'class489', 490: 'class490', 491: 'class491', 492: 'class492', 493: 'class493', 494: 'class494', 495: 'class495', 496: 'class496', 497: 'class497', 498: 'class498', 499: 'class499', 500: 'class500', 501: 'class501', 502: 'class502', 503: 'class503', 504: 'class504', 505: 'class505', 506: 'class506', 507: 'class507', 508: 'class508', 509: 'class509', 510: 'class510', 511: 'class511', 512: 'class512', 513: 'class513', 514: 'class514', 515: 'class515', 516: 'class516', 517: 'class517', 518: 'class518', 519: 'class519', 520: 'class520', 521: 'class521', 522: 'class522', 523: 'class523', 524: 'class524', 525: 'class525', 526: 'class526', 527: 'class527', 528: 'class528', 529: 'class529', 530: 'class530', 531: 'class531', 532: 'class532', 533: 'class533', 534: 'class534', 535: 'class535', 536: 'class536', 537: 'class537', 538: 'class538', 539: 'class539', 540: 'class540', 541: 'class541', 542: 'class542', 543: 'class543', 544: 'class544', 545: 'class545', 546: 'class546', 547: 'class547', 548: 'class548', 549: 'class549', 550: 'class550', 551: 'class551', 552: 'class552', 553: 'class553', 554: 'class554', 555: 'class555', 556: 'class556', 557: 'class557', 558: 'class558', 559: 'class559', 560: 'class560', 561: 'class561', 562: 'class562', 563: 'class563', 564: 'class564', 565: 'class565', 566: 'class566', 567: 'class567', 568: 'class568', 569: 'class569', 570: 'class570', 571: 'class571', 572: 'class572', 573: 'class573', 574: 'class574', 575: 'class575', 576: 'class576', 577: 'class577', 578: 'class578', 579: 'class579', 580: 'class580', 581: 'class581', 582: 'class582', 583: 'class583', 584: 'class584', 585: 'class585', 586: 'class586', 587: 'class587', 588: 'class588', 589: 'class589', 590: 'class590', 591: 'class591', 592: 'class592', 593: 'class593', 594: 'class594', 595: 'class595', 596: 'class596', 597: 'class597', 598: 'class598', 599: 'class599', 600: 'class600', 601: 'class601', 602: 'class602', 603: 'class603', 604: 'class604', 605: 'class605', 606: 'class606', 607: 'class607', 608: 'class608', 609: 'class609', 610: 'class610', 611: 'class611', 612: 'class612', 613: 'class613', 614: 'class614', 615: 'class615', 616: 'class616', 617: 'class617', 618: 'class618', 619: 'class619', 620: 'class620', 621: 'class621', 622: 'class622', 623: 'class623', 624: 'class624', 625: 'class625', 626: 'class626', 627: 'class627', 628: 'class628', 629: 'class629', 630: 'class630', 631: 'class631', 632: 'class632', 633: 'class633', 634: 'class634', 635: 'class635', 636: 'class636', 637: 'class637', 638: 'class638', 639: 'class639', 640: 'class640', 641: 'class641', 642: 'class642', 643: 'class643', 644: 'class644', 645: 'class645', 646: 'class646', 647: 'class647', 648: 'class648', 649: 'class649', 650: 'class650', 651: 'class651', 652: 'class652', 653: 'class653', 654: 'class654', 655: 'class655', 656: 'class656', 657: 'class657', 658: 'class658', 659: 'class659', 660: 'class660', 661: 'class661', 662: 'class662', 663: 'class663', 664: 'class664', 665: 'class665', 666: 'class666', 667: 'class667', 668: 'class668', 669: 'class669', 670: 'class670', 671: 'class671', 672: 'class672', 673: 'class673', 674: 'class674', 675: 'class675', 676: 'class676', 677: 'class677', 678: 'class678', 679: 'class679', 680: 'class680', 681: 'class681', 682: 'class682', 683: 'class683', 684: 'class684', 685: 'class685', 686: 'class686', 687: 'class687', 688: 'class688', 689: 'class689', 690: 'class690', 691: 'class691', 692: 'class692', 693: 'class693', 694: 'class694', 695: 'class695', 696: 'class696', 697: 'class697', 698: 'class698', 699: 'class699', 700: 'class700', 701: 'class701', 702: 'class702', 703: 'class703', 704: 'class704', 705: 'class705', 706: 'class706', 707: 'class707', 708: 'class708', 709: 'class709', 710: 'class710', 711: 'class711', 712: 'class712', 713: 'class713', 714: 'class714', 715: 'class715', 716: 'class716', 717: 'class717', 718: 'class718', 719: 'class719', 720: 'class720', 721: 'class721', 722: 'class722', 723: 'class723', 724: 'class724', 725: 'class725', 726: 'class726', 727: 'class727', 728: 'class728', 729: 'class729', 730: 'class730', 731: 'class731', 732: 'class732', 733: 'class733', 734: 'class734', 735: 'class735', 736: 'class736', 737: 'class737', 738: 'class738', 739: 'class739', 740: 'class740', 741: 'class741', 742: 'class742', 743: 'class743', 744: 'class744', 745: 'class745', 746: 'class746', 747: 'class747', 748: 'class748', 749: 'class749', 750: 'class750', 751: 'class751', 752: 'class752', 753: 'class753', 754: 'class754', 755: 'class755', 756: 'class756', 757: 'class757', 758: 'class758', 759: 'class759', 760: 'class760', 761: 'class761', 762: 'class762', 763: 'class763', 764: 'class764', 765: 'class765', 766: 'class766', 767: 'class767', 768: 'class768', 769: 'class769', 770: 'class770', 771: 'class771', 772: 'class772', 773: 'class773', 774: 'class774', 775: 'class775', 776: 'class776', 777: 'class777', 778: 'class778', 779: 'class779', 780: 'class780', 781: 'class781', 782: 'class782', 783: 'class783', 784: 'class784', 785: 'class785', 786: 'class786', 787: 'class787', 788: 'class788', 789: 'class789', 790: 'class790', 791: 'class791', 792: 'class792', 793: 'class793', 794: 'class794', 795: 'class795', 796: 'class796', 797: 'class797', 798: 'class798', 799: 'class799', 800: 'class800', 801: 'class801', 802: 'class802', 803: 'class803', 804: 'class804', 805: 'class805', 806: 'class806', 807: 'class807', 808: 'class808', 809: 'class809', 810: 'class810', 811: 'class811', 812: 'class812', 813: 'class813', 814: 'class814', 815: 'class815', 816: 'class816', 817: 'class817', 818: 'class818', 819: 'class819', 820: 'class820', 821: 'class821', 822: 'class822', 823: 'class823', 824: 'class824', 825: 'class825', 826: 'class826', 827: 'class827', 828: 'class828', 829: 'class829', 830: 'class830', 831: 'class831', 832: 'class832', 833: 'class833', 834: 'class834', 835: 'class835', 836: 'class836', 837: 'class837', 838: 'class838', 839: 'class839', 840: 'class840', 841: 'class841', 842: 'class842', 843: 'class843', 844: 'class844', 845: 'class845', 846: 'class846', 847: 'class847', 848: 'class848', 849: 'class849', 850: 'class850', 851: 'class851', 852: 'class852', 853: 'class853', 854: 'class854', 855: 'class855', 856: 'class856', 857: 'class857', 858: 'class858', 859: 'class859', 860: 'class860', 861: 'class861', 862: 'class862', 863: 'class863', 864: 'class864', 865: 'class865', 866: 'class866', 867: 'class867', 868: 'class868', 869: 'class869', 870: 'class870', 871: 'class871', 872: 'class872', 873: 'class873', 874: 'class874', 875: 'class875', 876: 'class876', 877: 'class877', 878: 'class878', 879: 'class879', 880: 'class880', 881: 'class881', 882: 'class882', 883: 'class883', 884: 'class884', 885: 'class885', 886: 'class886', 887: 'class887', 888: 'class888', 889: 'class889', 890: 'class890', 891: 'class891', 892: 'class892', 893: 'class893', 894: 'class894', 895: 'class895', 896: 'class896', 897: 'class897', 898: 'class898', 899: 'class899', 900: 'class900', 901: 'class901', 902: 'class902', 903: 'class903', 904: 'class904', 905: 'class905', 906: 'class906', 907: 'class907', 908: 'class908', 909: 'class909', 910: 'class910', 911: 'class911', 912: 'class912', 913: 'class913', 914: 'class914', 915: 'class915', 916: 'class916', 917: 'class917', 918: 'class918', 919: 'class919', 920: 'class920', 921: 'class921', 922: 'class922', 923: 'class923', 924: 'class924', 925: 'class925', 926: 'class926', 927: 'class927', 928: 'class928', 929: 'class929', 930: 'class930', 931: 'class931', 932: 'class932', 933: 'class933', 934: 'class934', 935: 'class935', 936: 'class936', 937: 'class937', 938: 'class938', 939: 'class939', 940: 'class940', 941: 'class941', 942: 'class942', 943: 'class943', 944: 'class944', 945: 'class945', 946: 'class946', 947: 'class947', 948: 'class948', 949: 'class949', 950: 'class950', 951: 'class951', 952: 'class952', 953: 'class953', 954: 'class954', 955: 'class955', 956: 'class956', 957: 'class957', 958: 'class958', 959: 'class959', 960: 'class960', 961: 'class961', 962: 'class962', 963: 'class963', 964: 'class964', 965: 'class965', 966: 'class966', 967: 'class967', 968: 'class968', 969: 'class969', 970: 'class970', 971: 'class971', 972: 'class972', 973: 'class973', 974: 'class974', 975: 'class975', 976: 'class976', 977: 'class977', 978: 'class978', 979: 'class979', 980: 'class980', 981: 'class981', 982: 'class982', 983: 'class983', 984: 'class984', 985: 'class985', 986: 'class986', 987: 'class987', 988: 'class988', 989: 'class989', 990: 'class990', 991: 'class991', 992: 'class992', 993: 'class993', 994: 'class994', 995: 'class995', 996: 'class996', 997: 'class997', 998: 'class998'}\n",
      "obb: None\n",
      "orig_img: array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8)\n",
      "orig_shape: (794, 496)\n",
      "path: 'G:\\\\Shahadath\\\\Dataset\\\\test\\\\images\\\\5096_0745204059_03_WRI-R1_F010.png'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 3.0090808868408203, 'inference': 38.36369514465332, 'postprocess': 1.0116100311279297}]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the optimized TensorRT model\n",
    "model = YOLO(\"G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine\")\n",
    "\n",
    "# Run inference on an image\n",
    "results = model(\"G:/Shahadath/Dataset/test/images/5096_0745204059_03_WRI-R1_F010.png\", verbose=False)\n",
    "\n",
    "# Display results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark Performance (Before vs After TensorRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Loading PyTorch Model...\n",
      "\n",
      "🔹 Loading TensorRT FP16 Model...\n",
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "\n",
      "🚀 Running Benchmark...\n",
      "✅ PyTorch Model - Mean: 23.11 ms | Min: 15.62 ms | Max: 467.21 ms\n",
      "Loading G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine for TensorRT inference...\n",
      "WARNING  Metadata not found for 'model=G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n",
      "✅ TensorRT FP16 Model - Mean: 16.15 ms | Min: 9.09 ms | Max: 305.39 ms\n",
      "\n",
      "🔥 Speedup with TensorRT FP16: 1.43x Faster 🚀\n",
      "\n",
      "📊 Benchmark Results:\n",
      "        Model  Avg Inference Time (ms) Speedup Factor\n",
      "      PyTorch                23.105781              -\n",
      "TensorRT FP16                16.146854          1.43x\n",
      "\n",
      "📊 Performance Comparison (PyTorch vs TensorRT FP16):\n",
      "\n",
      "                  Metric PyTorch  TensorRT FP16\n",
      "Mean Inference Time (ms)   23.11          16.15\n",
      " Min Inference Time (ms)   15.62           9.09\n",
      " Max Inference Time (ms)  467.21         305.39\n",
      "          Speedup Factor       -           1.43\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "pytorch_model_path = \"G:/Shahadath/Model/Model_M_best.pt\"  # Original PyTorch model\n",
    "tensorrt_model_path = \"G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine\"  # Optimized TensorRT FP16 model\n",
    "test_images_dir = \"G:/Shahadath/Dataset/test/images\"  # Path to test images\n",
    "\n",
    "# Get list of test images\n",
    "img_files = [f for f in os.listdir(test_images_dir) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "num_images = len(img_files)\n",
    "\n",
    "# ------------------ Function to Benchmark Model ------------------\n",
    "def benchmark_model(model, model_name):\n",
    "    \"\"\"Runs inference on all test images and calculates inference time statistics.\"\"\"\n",
    "    inference_times = []\n",
    "\n",
    "    for img_file in img_files:\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            _ = model(img_path, verbose=False)  # Run inference\n",
    "            inference_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "    # Compute mean, min, and max inference times\n",
    "    mean_time = sum(inference_times) / len(inference_times)\n",
    "    min_time = min(inference_times)\n",
    "    max_time = max(inference_times)\n",
    "\n",
    "    print(f\" {model_name} - Mean: {mean_time:.2f} ms | Min: {min_time:.2f} ms | Max: {max_time:.2f} ms\")\n",
    "\n",
    "    return mean_time, min_time, max_time\n",
    "\n",
    "# ------------------ Load Models ------------------\n",
    "print(\"\\n🔹 Loading PyTorch Model...\")\n",
    "pytorch_model = YOLO(pytorch_model_path)\n",
    "\n",
    "print(\"\\n🔹 Loading TensorRT FP16 Model...\")\n",
    "tensorrt_model = YOLO(tensorrt_model_path)\n",
    "\n",
    "# ------------------ Run Benchmark ------------------\n",
    "print(\"\\n🚀 Running Benchmark...\")\n",
    "\n",
    "# PyTorch Benchmark\n",
    "pytorch_mean, pytorch_min, pytorch_max = benchmark_model(pytorch_model, \"PyTorch Model\")\n",
    "\n",
    "# TensorRT FP16 Benchmark\n",
    "tensorrt_mean, tensorrt_min, tensorrt_max = benchmark_model(tensorrt_model, \"TensorRT FP16 Model\")\n",
    "\n",
    "# ------------------ Calculate Speedup ------------------\n",
    "speedup_factor = pytorch_mean / tensorrt_mean\n",
    "print(f\"\\n🔥 Speedup with TensorRT FP16: {speedup_factor:.2f}x Faster 🚀\")\n",
    "\n",
    "# ------------------ Display Results in Table ------------------\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"PyTorch\", \"TensorRT FP16\"],\n",
    "    \"Avg Inference Time (ms)\": [pytorch_mean, tensorrt_mean],\n",
    "    \"Speedup Factor\": [\"-\", f\"{speedup_factor:.2f}x\"]\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Benchmark Results:\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# --------------------- Performance Comparison ---------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean Inference Time (ms)\", \"Min Inference Time (ms)\", \"Max Inference Time (ms)\", \"Speedup Factor\"],\n",
    "    \"PyTorch\": [round(pytorch_mean, 2), round(pytorch_min, 2), round(pytorch_max, 2), \"-\"],\n",
    "    \"TensorRT FP16\": [round(tensorrt_mean, 2), round(tensorrt_min, 2), round(tensorrt_max, 2),\n",
    "                      round(speedup_factor, 2)]\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Performance Comparison (PyTorch vs TensorRT FP16):\\n\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.50  Python-3.9.13 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12287MiB)\n",
      "YOLO11m summary (fused): 303 layers, 20,036,971 parameters, 0 gradients, 67.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Shahadath\\Dataset\\test\\labels.cache... 2033 images, 1 backgrounds, 0 corrupt: 100%|██████████| 2033/2033 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 128/128 [00:15<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2033       4720      0.843       0.68      0.836       0.57\n",
      "           boneanomaly         16         19       0.81      0.526      0.615      0.431\n",
      "            bonelesion          8          8      0.926      0.625      0.735      0.463\n",
      "           foreignbody          1          1          1          0      0.995      0.597\n",
      "              fracture       1351       1784      0.931       0.95      0.977      0.666\n",
      "                 metal         74         85       0.96          1      0.995      0.884\n",
      "    periostealreaction        230        352      0.752      0.699      0.775      0.439\n",
      "          pronatorsign         47         47      0.699      0.745      0.818      0.508\n",
      "            softtissue         38         39      0.533       0.59      0.619      0.392\n",
      "                  text       2029       2385      0.976      0.987      0.994      0.755\n",
      "Speed: 0.1ms preprocess, 3.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n",
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.50  Python-3.9.13 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12287MiB)\n",
      "Loading G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine for TensorRT inference...\n",
      "WARNING  Metadata not found for 'model=G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Shahadath\\Dataset\\test\\labels.cache... 2033 images, 1 backgrounds, 0 corrupt: 100%|██████████| 2033/2033 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2033/2033 [00:17<00:00, 114.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2033       4720      0.843      0.658      0.837      0.571\n",
      "           boneanomaly         16         19      0.751      0.477      0.605      0.427\n",
      "            bonelesion          8          8      0.867        0.5      0.716      0.466\n",
      "           foreignbody          1          1          1          0      0.995      0.597\n",
      "              fracture       1351       1784      0.936      0.948      0.976      0.666\n",
      "                 metal         74         85      0.942          1      0.995       0.88\n",
      "    periostealreaction        230        352      0.777      0.682      0.778      0.439\n",
      "          pronatorsign         47         47      0.768      0.766      0.816      0.505\n",
      "            softtissue         38         39      0.566      0.569      0.659      0.408\n",
      "                  text       2029       2385      0.977      0.984      0.993      0.753\n",
      "Speed: 0.3ms preprocess, 3.3ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val10\u001b[0m\n",
      "Loading G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine for TensorRT inference...\n",
      "WARNING  Metadata not found for 'model=G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n",
      "\n",
      " Model Evaluation Results:\n",
      "\n",
      "                      Model  Precision  Recall  mAP@50  mAP@50-95 Params(M) FLOPs(B)  Min Inference Time (ms)  Max Inference Time (ms)  Mean Inference Time (ms)\n",
      "            Model_M_best.pt      0.843   0.680   0.836      0.570    20.037   40.074                    15.51                    92.99                     22.46\n",
      "Model_M_best_L1_Pruned_fp16      0.843   0.658   0.837      0.571       N/A      N/A                     8.10                   409.78                     15.78\n",
      "\n",
      " Performance Comparison (PyTorch vs TensorRT FP16):\n",
      "\n",
      "                  Metric PyTorch TensorRT FP16\n",
      "Mean Inference Time (ms)   22.46         15.78\n",
      " Min Inference Time (ms)     8.1           8.1\n",
      " Max Inference Time (ms)  409.78        409.78\n",
      "          Speedup Factor       -         1.42x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define the models and their paths\n",
    "models_info = [\n",
    "    {\n",
    "        \"name\": \"Model_M_best.pt\",\n",
    "        \"weights_path\": \"Model/Model_M_best.pt\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Model_M_best_L1_Pruned_fp16\",\n",
    "        \"weights_path\": \"G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Path to the test images\n",
    "test_images_dir = 'Dataset/test/images'\n",
    "\n",
    "# Path to data.yaml\n",
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "# Get list of image files in the test directory\n",
    "img_files = [f for f in os.listdir(test_images_dir) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "num_images = len(img_files)\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "inference_times = {}  # Store inference times for speedup calculation\n",
    "\n",
    "# Iterate over each model and evaluate\n",
    "for model_info in models_info:\n",
    "    model_name = model_info[\"name\"]\n",
    "    weights_path = model_info[\"weights_path\"]\n",
    "    \n",
    "    # Load the YOLO model\n",
    "    model = YOLO(weights_path)\n",
    "    \n",
    "    # -------------------- Model Evaluation --------------------\n",
    "    # Evaluate the model on the test set\n",
    "    metrics = model.val(data=data_yaml_path, split=\"test\", batch=16, imgsz=640)\n",
    "\n",
    "    # Extract metrics safely\n",
    "    try:\n",
    "        precision = metrics.box.p.mean()  # Mean Precision\n",
    "        recall = metrics.box.r.mean()     # Mean Recall\n",
    "        map50 = metrics.box.map50.mean()  # Mean mAP@50\n",
    "        map = metrics.box.map.mean()      # Mean mAP@50-95\n",
    "    except AttributeError:\n",
    "        print(f\"Error extracting metrics for {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # -------------------- Inference Time Calculation --------------------\n",
    "    inference_times_list = []  # Store inference times for min/max calculation\n",
    "\n",
    "    # Loop through each image file and perform inference\n",
    "    for img_file in img_files:\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "\n",
    "        # Suppress output from YOLO during inference\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            _ = model(img_path, verbose=False)  # Perform inference with verbose=False\n",
    "            inference_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        inference_times_list.append(inference_time)\n",
    "\n",
    "    # Compute min, max, and mean inference times\n",
    "    avg_inference_time = sum(inference_times_list) / num_images\n",
    "    min_inference_time = min(inference_times_list)\n",
    "    max_inference_time = max(inference_times_list)\n",
    "\n",
    "    # Store inference times for speedup calculation\n",
    "    inference_times[model_name] = avg_inference_time\n",
    "\n",
    "    # -------------------- Model Parameters and FLOPs --------------------\n",
    "    # Extracting model parameters (in millions)\n",
    "    try:\n",
    "        params = sum(p.numel() for p in model.parameters()) / 1e6  # In millions\n",
    "        flops = params * 2  # Rough estimate of FLOPs (billions)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating parameters/FLOPs for {model_name}: {e}\")\n",
    "        params, flops = 0, 0  # Set to 0 in case of an error\n",
    "\n",
    "    # Use \"N/A\" if params or flops are zero\n",
    "    params_str = \"N/A\" if params == 0 else round(params, 3)\n",
    "    flops_str = \"N/A\" if flops == 0 else round(flops, 3)\n",
    "\n",
    "    # Append the results for this model\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": round(precision, 3),\n",
    "        \"Recall\": round(recall, 3),\n",
    "        \"mAP@50\": round(map50, 3),\n",
    "        \"mAP@50-95\": round(map, 3),\n",
    "        \"Params(M)\": params_str,\n",
    "        \"FLOPs(B)\": flops_str,\n",
    "        \"Min Inference Time (ms)\": round(min_inference_time, 2),\n",
    "        \"Max Inference Time (ms)\": round(max_inference_time, 2),\n",
    "        \"Mean Inference Time (ms)\": round(avg_inference_time, 2)\n",
    "    })\n",
    "\n",
    "# ------------------ Calculate Speedup ------------------\n",
    "try:\n",
    "    pytorch_mean = inference_times[\"Model_M_best.pt\"]\n",
    "    tensorrt_mean = inference_times[\"Model_M_best_L1_Pruned_fp16\"]\n",
    "    speedup_factor = pytorch_mean / tensorrt_mean\n",
    "    speedup_str = f\"{speedup_factor:.2f}x\"\n",
    "except KeyError:\n",
    "    speedup_str = \"N/A\"\n",
    "\n",
    "# ------------------ Display Results ------------------\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n Model Evaluation Results:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# --------------------- Performance Comparison ---------------------\n",
    "df_comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean Inference Time (ms)\", \"Min Inference Time (ms)\", \"Max Inference Time (ms)\", \"Speedup Factor\"],\n",
    "    \"PyTorch\": [round(pytorch_mean, 2), round(min_inference_time, 2), round(max_inference_time, 2), \"-\"],\n",
    "    \"TensorRT FP16\": [round(tensorrt_mean, 2), round(min_inference_time, 2), round(max_inference_time, 2), speedup_str]\n",
    "})\n",
    "\n",
    "print(\"\\n Performance Comparison (PyTorch vs TensorRT FP16):\\n\")\n",
    "print(df_comparison.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.19.2)\n",
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thop) (2.6.0+cu118)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (2024.10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch->thop) (2.1.3)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install onnx onnxruntime thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.6.0+cu118)\n",
      "Collecting ptflops\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: ptflops\n",
      "Successfully installed ptflops-0.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install onnx onnxruntime torch ptflops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.15.0)\n",
      "Collecting onnx2torch\n",
      "  Downloading onnx2torch-1.5.15-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: ptflops in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading onnx2torch-1.5.15-py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 0.0/81.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.0/81.0 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: onnx2torch\n",
      "Successfully installed onnx2torch-1.5.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install onnx onnx2torch ptflops torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "Total Parameters: 20.060M\n",
      "Total FLOPs: 34.112G\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"Model/Model_M_L1_Pruned.pt\")\n",
    "\n",
    "# Define static input shape (batch_size=1, channels=3, height=640, width=640)\n",
    "input_shape = (1, 3, 640, 640)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "\n",
    "# Get the PyTorch model from YOLO\n",
    "pytorch_model = model.model  # Extract underlying PyTorch model\n",
    "\n",
    "# Compute FLOPs and Parameters\n",
    "flops, params = profile(pytorch_model, inputs=(dummy_input,))\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Parameters: {params / 1e6:.3f}M\")  # Convert to Million (M)\n",
    "print(f\"Total FLOPs: {flops / 1e9:.3f}G\")  # Convert to Giga (G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.50  Python-3.9.13 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12287MiB)\n",
      "Loading G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine for TensorRT inference...\n",
      "WARNING  Metadata not found for 'model=G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Shahadath\\Dataset\\test\\labels.cache... 2033 images, 1 backgrounds, 0 corrupt: 100%|██████████| 2033/2033 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2033/2033 [00:16<00:00, 121.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2033       4720      0.843      0.658      0.837      0.571\n",
      "           boneanomaly         16         19      0.751      0.477      0.605      0.427\n",
      "            bonelesion          8          8      0.867        0.5      0.716      0.466\n",
      "           foreignbody          1          1          1          0      0.995      0.597\n",
      "              fracture       1351       1784      0.936      0.948      0.976      0.666\n",
      "                 metal         74         85      0.942          1      0.995       0.88\n",
      "    periostealreaction        230        352      0.777      0.682      0.778      0.439\n",
      "          pronatorsign         47         47      0.768      0.766      0.816      0.505\n",
      "            softtissue         38         39      0.566      0.569      0.659      0.408\n",
      "                  text       2029       2385      0.977      0.984      0.993      0.753\n",
      "Speed: 0.3ms preprocess, 3.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "Precision: 0.843\n",
      "Recall: 0.658\n",
      "mAP@50: 0.837\n",
      "mAP@50-95: 0.571\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Paths to required files\n",
    "weights_path = 'G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n",
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "metrics = model.val(data=data_yaml_path, split=\"test\", batch=16, imgsz=640)\n",
    "\n",
    "# Extract and print metrics\n",
    "precision = metrics.box.p.mean()  # Mean Precision\n",
    "recall = metrics.box.r.mean()     # Mean Recall\n",
    "map50 = metrics.box.map50.mean()  # Mean mAP@50\n",
    "map = metrics.box.map.mean()      # Mean mAP@50-95\n",
    "\n",
    "print(f\"Precision: {round(precision, 3)}\")\n",
    "print(f\"Recall: {round(recall, 3)}\")\n",
    "print(f\"mAP@50: {round(map50, 3)}\")\n",
    "print(f\"mAP@50-95: {round(map, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.50  Python-3.9.13 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12287MiB)\n",
      "Loading G:\\Shahadath\\Model\\Model_M_best_L1_Pruned_fp16.engine for TensorRT inference...\n",
      "WARNING  Metadata not found for 'model=G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Shahadath\\Dataset\\val\\labels.cache... 4066 images, 4 backgrounds, 0 corrupt: 100%|██████████| 4066/4066 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4066/4066 [00:35<00:00, 113.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4066       9500      0.808      0.755       0.83      0.584\n",
      "           boneanomaly         46         71      0.694      0.549      0.634      0.381\n",
      "            bonelesion          9          9      0.728      0.556      0.766      0.462\n",
      "           foreignbody          4          4       0.85          1      0.995      0.858\n",
      "              fracture       2736       3653      0.945       0.94       0.98      0.663\n",
      "                 metal        128        148      0.954      0.966      0.991      0.873\n",
      "    periostealreaction        421        660      0.787      0.673      0.777      0.435\n",
      "          pronatorsign        125        126      0.762      0.627      0.778      0.482\n",
      "            softtissue         72         77      0.575      0.506      0.552      0.345\n",
      "                  text       4055       4752      0.976      0.979      0.993      0.754\n",
      "Speed: 0.3ms preprocess, 3.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Precision: 0.808\n",
      "Recall: 0.755\n",
      "mAP@50: 0.83\n",
      "mAP@50-95: 0.584\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Paths to required files\n",
    "weights_path = 'G:/Shahadath/Model/Model_M_best_L1_Pruned_fp16.engine'\n",
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# Evaluate the model on the val set\n",
    "metrics = model.val(data=data_yaml_path, split=\"val\", batch=16, imgsz=640)\n",
    "\n",
    "# Extract and print metrics\n",
    "precision = metrics.box.p.mean()  # Mean Precision\n",
    "recall = metrics.box.r.mean()     # Mean Recall\n",
    "map50 = metrics.box.map50.mean()  # Mean mAP@50\n",
    "map = metrics.box.map.mean()      # Mean mAP@50-95\n",
    "\n",
    "print(f\"Precision: {round(precision, 3)}\")\n",
    "print(f\"Recall: {round(recall, 3)}\")\n",
    "print(f\"mAP@50: {round(map50, 3)}\")\n",
    "print(f\"mAP@50-95: {round(map, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
